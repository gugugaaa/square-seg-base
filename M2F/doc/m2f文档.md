数据集
coco/
  annotations/
    instances_{train,val}2017.json
    panoptic_{train,val}2017.json
  {train,val}2017/
    # image files that are mentioned in the corresponding json
  panoptic_{train,val}2017/  # png annotations
  panoptic_semseg_{train,val}2017/  # generated by the script mentioned below

安装
PyTorch ≥ 1.9, Python ≥ 3.6, Detectron2
```bash
# 根据经验，这一段要求不严格，直接用我服务器自带的torch2.0.0即可
# conda create --name mask2former python=3.8 -y
# conda activate mask2former
# conda install pytorch==1.9.0 torchvision==0.10.0 cudatoolkit=11.1 -c pytorch -c nvidia
pip install -U opencv-python

# under your working directory
git clone git@github.com:facebookresearch/detectron2.git
cd detectron2
pip install -e .
pip install git+https://github.com/cocodataset/panopticapi.git
pip install git+https://github.com/mcordts/cityscapesScripts.git

cd ..
git clone git@github.com:facebookresearch/Mask2Former.git
cd Mask2Former
pip install -r requirements.txt
cd mask2former/modeling/pixel_decoder/ops
# CUDA kernel for MSDeformAttn
sh make.sh
```

推理
```python
from transformers import AutoImageProcessor, Mask2FormerForUniversalSegmentation

processor = AutoImageProcessor.from_pretrained("facebook/mask2former-swin-large-coco-instance")
model = Mask2FormerForUniversalSegmentation.from_pretrained("facebook/mask2former-swin-large-coco-instance")

inputs = processor(images=image, return_tensors="pt")
for k,v in inputs.items():
  print(k,v.shape)

import torch

# forward pass
with torch.no_grad():
  outputs = model(**inputs)

# you can pass them to processor for postprocessing
results = processor.post_process_instance_segmentation(outputs, target_sizes=[image.size[::-1]], threshold=0.9)[0]
print(results.keys())

for segment in results['segments_info']:
  print(segment)

import numpy as np

def get_mask(segment_id):
  print("Visualizing mask for:", model.config.id2label[segment_to_label[segment_id]])

  mask = (results['segmentation'].numpy() == segment_id)
  visual_mask = (mask * 255).astype(np.uint8)
  visual_mask = Image.fromarray(visual_mask)

  return visual_mask

# note: segment with id == 0 means "background",
# so we visualize segment with id == 1 here
get_mask(segment_id=1)
```

模型库
R50 [r50](https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/instance/maskformer2_R50_bs16_50ep/model_final_3c8ec9.pkl)
Swin-T [swin-tiny](https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/instance/maskformer2_swin_tiny_bs16_50ep/model_final_86143f.pkl)
Swin-B-IN21K [swin-base](https://dl.fbaipublicfiles.com/maskformer/mask2former/coco/instance/maskformer2_swin_base_IN21k_384_bs16_50ep/model_final_83d103.pkl)

训练（使用Trainer）
```bash
python run_instance_segmentation.py \
    --model_name_or_path facebook/mask2former-swin-tiny-coco-instance \
    --output_dir finetune-instance-segmentation-ade20k-mini-mask2former \
    --dataset_name qubvel-hf/ade20k-mini \
    --do_reduce_labels \
    --image_height 256 \
    --image_width 256 \
    --do_train \
    --fp16 \
    --num_train_epochs 40 \
    --learning_rate 1e-5 \
    --lr_scheduler_type constant \
    --per_device_train_batch_size 8 \
    --gradient_accumulation_steps 2 \
    --dataloader_num_workers 8 \
    --dataloader_persistent_workers \
    --dataloader_prefetch_factor 4 \
    --do_eval \
    --eval_strategy epoch \
    --logging_strategy epoch \
    --save_strategy epoch \
    --save_total_limit 2 \
    --push_to_hub
```
[其他训练参数](https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments)

推荐使用 HuggingFace Dataset
创建数据集
```python
from datasets import Dataset, DatasetDict
from datasets import Image as DatasetImage

label2id = {
    "background": 0,
    "person": 1,
    "car": 2,
}

train_split = {
    "image": [<PIL Image 1>, <PIL Image 2>, <PIL Image 3>, ...],
    "annotation": [<PIL Image ann 1>, <PIL Image ann 2>, <PIL Image ann 3>, ...],
}

validation_split = {
    "image": [<PIL Image 101>, <PIL Image 102>, <PIL Image 103>, ...],
    "annotation": [<PIL Image ann 101>, <PIL Image ann 102>, <PIL Image ann 103>, ...],
}

def create_instance_segmentation_dataset(label2id, **splits):
    dataset_dict = {}
    for split_name, split in splits.items():
        split["semantic_class_to_id"] = [label2id] * len(split["image"])
        dataset_split = (
            Dataset.from_dict(split)
            .cast_column("image", DatasetImage())
            .cast_column("annotation", DatasetImage())
        )
        dataset_dict[split_name] = dataset_split
    return DatasetDict(dataset_dict)

dataset = create_instance_segmentation_dataset(label2id, train=train_split, validation=validation_split)
dataset.push_to_hub("qubvel-hf/ade20k-nano")
# local_dataset_path = "./my_local_ade20k_nano"
# dataset.save_to_disk(local_dataset_path)
```

您需要遍历您所有的 COCO 标注（polygons）：
初始化： 创建三个 $H \times W$ 的全零数组（比如用 numpy.zeros((H, W), dtype=np.uint8)），分别叫 channel_1、channel_2、channel_3。
遍历实例： 假设您的图片有两个正方形（两个 polygon）。实例 1 (正方形 A)：将这个 polygon 画在 channel_1 上，填充值为 1 (语义 ID)。将这个 polygon 画在 channel_2 上，填充值为 1 (实例 ID 1)。实例 2 (正方形 B)：将这个 polygon 画在 channel_1 上，填充值为 1 (语义 ID)。将这个 polygon 画在 channel_2 上，填充值为 2 (实例 ID 2)。合并： 使用 numpy.stack([channel_1, channel_2, channel_3], axis=-1) 将它们合并成一个 $H \times W \times 3$ 的数组。
保存： 使用 PIL.Image.fromarray(merged_array, mode='RGB') 将这个数组转换为 PIL 图像，并保存为 annotation.png